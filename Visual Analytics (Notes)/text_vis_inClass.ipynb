{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text  Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim, spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "#NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('VoxData.csv', header=0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6903, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_on</th>\n",
       "      <th>slug</th>\n",
       "      <th>blurb</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Every year of a prison term makes a couple 32 ...</td>\n",
       "      <td>Dara Lind</td>\n",
       "      <td>Criminal Justice</td>\n",
       "      <td>2014-05-29 12:30:05</td>\n",
       "      <td>2014-05-29 12:30:07</td>\n",
       "      <td>http://www.vox.com/2014/5/29/5756646/every-yea...</td>\n",
       "      <td>But even a short jail stay can strain a marria...</td>\n",
       "      <td>A new study by criminologists Sonja Siennick a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of Donald Trump</td>\n",
       "      <td>John Patty</td>\n",
       "      <td>Mischiefs of Faction</td>\n",
       "      <td>2016-01-12 19:50:08</td>\n",
       "      <td>2016-01-12 19:50:09</td>\n",
       "      <td>http://www.vox.com/mischiefs-of-faction/2016/1...</td>\n",
       "      <td>Social science predicted that it can't predict...</td>\n",
       "      <td>The current fight for the GOP presidential nom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acting white: the most insidious myth about bl...</td>\n",
       "      <td>JenÃ©e Desmond-Harris</td>\n",
       "      <td>Race in America</td>\n",
       "      <td>2015-03-04 13:40:02</td>\n",
       "      <td>2015-05-04 02:51:51</td>\n",
       "      <td>http://www.vox.com/2015/3/4/8138739/acting-whi...</td>\n",
       "      <td>This popular theory about how African-American...</td>\n",
       "      <td>You've probably heard it before: too many blac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hillary Clintonâ€™s pitch: Tim Kaine will be t...</td>\n",
       "      <td>Dylan Matthews</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-07-23 21:23:13</td>\n",
       "      <td>2016-07-25 15:56:38</td>\n",
       "      <td>http://www.vox.com/2016/7/23/12263516/tim-kain...</td>\n",
       "      <td>He's not Tom Perez or Cory Booker. But...</td>\n",
       "      <td>To many on the left, Tim Kaine’s selection as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democratic debate 2015: start time, schedule, ...</td>\n",
       "      <td>Andrew Prokop</td>\n",
       "      <td>Debates</td>\n",
       "      <td>2015-11-13 16:20:02</td>\n",
       "      <td>2015-11-14 23:47:28</td>\n",
       "      <td>http://www.vox.com/2015/11/13/9728432/democrat...</td>\n",
       "      <td>The three remaining candidates will debate in ...</td>\n",
       "      <td>The horrific attacks in Paris will loom large ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                 author  \\\n",
       "0  Every year of a prison term makes a couple 32 ...              Dara Lind   \n",
       "1                       Making sense of Donald Trump             John Patty   \n",
       "2  Acting white: the most insidious myth about bl...  JenÃ©e Desmond-Harris   \n",
       "3  Hillary Clintonâ€™s pitch: Tim Kaine will be t...         Dylan Matthews   \n",
       "4  Democratic debate 2015: start time, schedule, ...          Andrew Prokop   \n",
       "\n",
       "               category       published_date           updated_on  \\\n",
       "0      Criminal Justice  2014-05-29 12:30:05  2014-05-29 12:30:07   \n",
       "1  Mischiefs of Faction  2016-01-12 19:50:08  2016-01-12 19:50:09   \n",
       "2       Race in America  2015-03-04 13:40:02  2015-05-04 02:51:51   \n",
       "3       Hillary Clinton  2016-07-23 21:23:13  2016-07-25 15:56:38   \n",
       "4               Debates  2015-11-13 16:20:02  2015-11-14 23:47:28   \n",
       "\n",
       "                                                slug  \\\n",
       "0  http://www.vox.com/2014/5/29/5756646/every-yea...   \n",
       "1  http://www.vox.com/mischiefs-of-faction/2016/1...   \n",
       "2  http://www.vox.com/2015/3/4/8138739/acting-whi...   \n",
       "3  http://www.vox.com/2016/7/23/12263516/tim-kain...   \n",
       "4  http://www.vox.com/2015/11/13/9728432/democrat...   \n",
       "\n",
       "                                               blurb  \\\n",
       "0  But even a short jail stay can strain a marria...   \n",
       "1  Social science predicted that it can't predict...   \n",
       "2  This popular theory about how African-American...   \n",
       "3          He's not Tom Perez or Cory Booker. But...   \n",
       "4  The three remaining candidates will debate in ...   \n",
       "\n",
       "                                                body  \n",
       "0  A new study by criminologists Sonja Siennick a...  \n",
       "1  The current fight for the GOP presidential nom...  \n",
       "2  You've probably heard it before: too many blac...  \n",
       "3  To many on the left, Tim Kaine’s selection as ...  \n",
       "4  The horrific attacks in Paris will loom large ...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' distribution of authors'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' distribution of authors'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'altair' has no attribute 'Char'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-08e16a42c93e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m alt.Char(categories).mark_bar().encode(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'altair' has no attribute 'Char'"
     ]
    }
   ],
   "source": [
    "''' distribution of categories '''\n",
    "\n",
    "categories = df.groupby(['category']).size().reset_index(name='count')\n",
    "\n",
    "alt.Char(categories).mark_bar().encode(\n",
    "    x = alt.X()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category']=='Politics & Policy']['body'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing for Filtering and Machine Learning Models\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' we focus on the category of culture here '''\n",
    "processed = df[df['category']=='Politics & Policy'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' first trial of tokenization using simple_preprocess '''\n",
    "data_words = gensim.utils.simple_preprocess(processed['body'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word_list = [simple_preprocess(sentence) for sentence in processed['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of data_word_list: \" , len(data_word_list))\n",
    "print(\"length of data_word_list[0]: \" , len(data_word_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['com', 'from', 'subject', 're', 'edu', 'use', 'not', 'would', \n",
    "                   'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', \n",
    "                   'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', \n",
    "                   'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', \n",
    "                   'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = [[word for word in doc if word not in stop_words] for doc in data_word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of data_words: \" , len(data_words))\n",
    "print(\"length of data_words[0]: \" , len(data_words[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming using porter Stemming Algorithm\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "p = PorterStemmer()\n",
    "\n",
    "data_ready = []\n",
    "\n",
    "for text in data_words:\n",
    "    data_stemmed = p.stem_documents(text)\n",
    "    data_ready.append(data_stemmed)\n",
    "# data_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "too slow, do not run here\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "for sent in data_words:\n",
    "    # Parse the sentence using the loaded 'en' model object `nlp`. Extract the lemma for each token and join\n",
    "    doc = nlp(\" \".join(sent)) \n",
    "    data_ready.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "# remove stopwords once more after lemmatization\n",
    "data_ready = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_ready]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.sklearn_api import TfIdfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "model = TfIdfTransformer(dictionary=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "num_docs = id2word.num_docs\n",
    "num_terms = len(id2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus[:1]:\n",
    "    print([[id, id2word[id], freq] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = model.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an array of tf-idf vectors\n",
    "from gensim.matutils import corpus2dense, corpus2csc\n",
    "\n",
    "corpus_tfidf_dense = corpus2dense(tfidf_corpus, num_terms, num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus_tfidf_dense[corpus_tfidf_dense.max(axis=1) > 0.1]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering & Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_pca = PCA(n_components=2).fit_transform(X.T)\n",
    "result_tsne = TSNE(n_components=2, perplexity=10).fit_transform(X.T)\n",
    "\n",
    "tsne_df = pd.DataFrame(data=result_tsne, columns=['x','y'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for id in id2word.keys():\n",
    "    words.append(id2word[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = pd.DataFrame(data=corpus_tfidf_dense, index=words)\n",
    "mat = mat[mat.max(axis=1) > 0.1]\n",
    "\n",
    "wordtfidf = pd.DataFrame(data=mat.values.T, columns=mat.index)\n",
    "wordtfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.max(axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' distribution of authors'''\n",
    "author_count = processed.groupby('author').size().reset_index(name='count')\n",
    "\n",
    "alt.Chart(author_count).mark_bar().encode(\n",
    "    x = alt.X('author:N',  sort='-y'),\n",
    "    y = alt.Y('count:Q'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_count = author_count.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtfidf['author'] = processed['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_key_words = []\n",
    "\n",
    "''' key words for top 10 authors'''\n",
    "for author in author_count['author'][:10]:\n",
    "    # get the sum tf-idf for each word, do sum() across rows for each column\n",
    "    group_df = wordtfidf[wordtfidf['author'] == author].mean(axis=0)\n",
    "    # sort the tf-idf values\n",
    "    to_sort = [{'freq': group_df[x], 'word': x} for x in group_df.index]\n",
    "    sorted(to_sort, key=lambda d: d['freq'], reverse=True)\n",
    "    # add the words to the list\n",
    "    for i in range(10):\n",
    "        author_key_words.append([author, to_sort[i]['word'], to_sort[i]['freq']])\n",
    "        \n",
    "author_keyword_df = pd.DataFrame(data=group_key_words, columns=['author', 'keyword', 'tfidf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_keyword_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
